Технический отчёт — проект «Gender & Age Detection"
Дата анализа: 2025-12-01T13:37:52.304Z

Содержание
1. Полный стек технологий
2. Описание работы проекта (пошагово)
3. Реальные результаты и метрики
4. Ограничения
5. Рекомендуемые улучшения
6. Быстрые инструкции для проверки

---

1. Полный стек технологий
- Язык программирования: Python (предполагаемая поддержка современных версий — используемые требования указывают на совместимость с Python 3.10+ / 3.11+, requirements содержат пакеты для новейших версий).
- Библиотеки Python (ключевые, перечислены в requirements.txt):
  - tensorflow (tensorflow>=2.20.0) / tensorflow.keras — основная DL-платформа для обучения и инференса
  - keras (в подмодулях age_estimation используются keras напрямую)
  - numpy, pandas, scipy
  - opencv-python (cv2) — загрузка/обработка изображений, отображение результатов, сохранение кропов
  - dlib — детекция лиц (dlib.get_frontal_face_detector и иногда CNN-детектор)
  - scikit-learn (train_test_split и пр.)
  - albumentations — аугментации (упоминается в requirements)
  - omegaconf, hydra-core — конфигурация/CLI
  - wandb — интеграция логирования/экспериментов
  - tqdm — прогресс бары
  - h5py — работа с файлами весов
  - pytest, black, flake8 — тесты и стиль
- Модели и архитектуры (используемые в кодовой базе):
  - EfficientNetB3 (через tensorflow.keras.applications.EfficientNetB3) — используется как бэкбон в современной части проекта (src)
  - ResNet50 и InceptionResNetV2 (в age_estimation под-проекте)
  - В архитектуре (src/factory.py) backbone (без top), pooling='avg', два выходных «хэда»: pred_gender (Dense(2, softmax)) и pred_age (Dense(101, softmax)).
  - В age_estimation под-проекте есть модель с одним выходом pred_age (101 softmax) — возраст как распределение.
- Датасеты, упомянутые в проекте:
  - APPA-REAL (apparel/age apparent dataset) — упоминается для финетюнинга/обучения
  - UTKFace — используется в подготовке датасета (скрипты utkface/)
  - LAP / LAP dataset (упоминания в папке lap/)
- Инструменты компьютерного зрения:
  - OpenCV (cv2) — чтение изображений, ресайз, border, отображение GUI
  - dlib.get_frontal_face_detector() — HOG-подход по умолчанию для детекции лиц; в некоторых скриптах присутствует поддержка dlib.cnn_face_detection_model_v1 (скачиваемая CNN-модель dlib)
  - В проекте нет MTCNN/RetinaFace в явном виде, но есть возможность расширять детектор (код готов к смене детектора)

---

2. Полное описание работы проекта (пошагово)
2.1. Как загружается изображение
- Для пакетного режима (src/inference.py): функция predict_folder перебирает файлы в указанной директории (image_dir), читает их через cv2.imread (BGR), затем конвертирует в RGB (cv2.cvtColor(img, cv2.COLOR_BGR2RGB)).
- Для демо (demo.py / age_estimation/demo.py): поддерживается либо поток с веб-камеры (cv2.VideoCapture) либо чтение изображений из директории через yield_images_from_dir.

2.2. Как производится детекция лица
- Используется dlib.get_frontal_face_detector() (HOG-based). В некоторых вспомогательных скриптах (lap/create_lap_dataset.py) встречается использование dlib.cnn_face_detection_model_v1 при наличии скачанного веса.
- Детектор возвращает прямоугольники (left, top, right, bottom), код применяет margin (параметр --margin, default=0.4) для расширения бокса перед кропом.

2.3. Как происходит предобработка
- После кропа лицо ресайзится до img_size (извлекается из имени файла weight_file или из cfg.model.img_size).
- Если для backbone доступна функция preprocess_input (src/factory.get_preprocess_fn получает applications.<model>.preprocess_input), она применяется к каждому кропу.
- Если preprocess_fn отсутствует или выбрасывает исключение, происходит fallback: приведение типов к float32 и масштабирование в диапазон 0..1 (faces_np / 255.0).
- Для обучения используется ImageSequence генератор (src/generator.py) и, при необходимости, дополнительные аугментации (albumentations упоминается в requirements).

2.4. Метод определения пола
- Пол определяется классификацией: последний слой pred_gender — Dense(2, activation='softmax'). При инференсе код берёт predicted_genders[i][0] < 0.5 -> 'M' иначе 'F' (т.е. предполагается порядок классов [M, F]).
- Для обучения используется categorical_crossentropy для пола.

2.5. Метод определения возраста
- Возраст представлен как распределение по 101 классу (0..100). pred_age — Dense(101, softmax).
- При инференсе рассчитывается ожидаемое значение возраста: expected_age = dot(pred_age, np.arange(0,101)). В train.py также реализован смесь потерь: KLD между распределениями + MAE между ожидаемыми значениями.
- В age_estimation под-проекте реализованы модели age-only (ResNet50 / InceptionResNetV2) с output 101 классов.

2.6. Какие файлы за что отвечают (ключевые)
- demo.py — интерактивное демо (веб-камера или image_dir), отображение результатов с OpenCV.
- src/inference.py — пакетная инференция, батчинг лиц, сохранение кропов и CSV с результатами (results.csv).
- train.py — основной скрипт обучения (использует src/generator, src/factory, callbacks, mixed loss для возраста).
- src/factory.py — создание модели из keras.applications, сборка хедов pred_gender и pred_age, получение preprocess_fn и оптимизатора/плана обучения.
- src/generator.py — генератор ImageSequence (подготовка батчей для обучения).
- age_estimation/* — старый/альтернативный под-проект (модели age-only, скрипты подготовки данных для UTKFace/APPA-REAL).
- evaluate_predictions.py — утилита для расчёта метрик (MAE по возрасту, точность пола) при наличии meta CSV с ground truth.
- requirements.txt — зависимости.
- tests/ — базовые тесты (например, тесты генератора).

---

3. Реальные результаты работы
- Формат выводов: results.csv c колонками [filename, face_index, age, gender, x1, y1, x2, y2]. Также опционально создаются кропы в predicted_crops/.
- Примеры предсказаний: (пример формата)
  - image1.jpg, 0, 29, M, x1, y1, x2, y2
  - image2.jpg, 0, 45, F, x1, y1, x2, y2
- Метрики: проект содержит evaluate_predictions.py, который может сравнить results.csv с meta/<db>.csv и посчитать mae_age и gender_accuracy при условии, что в meta CSV есть колонки с GT (ages, genders или подобные). По умолчанию в репозитории нет заранее посчитанных метрик — если вы запустите evaluate_predictions с корректным meta CSV, он создаст metrics.json.
- В репозитории на момент анализа нет общего отчёта с агрегированными MAE/accuracy — значит, метрики не предрасчитаны автоматически при установке; их можно получить запустив обучение/оценку на выбранном датасете и затем evaluate_predictions.

---

4. Ограничения проекта
- Детекция лиц (dlib HOG) чувствительна к экстремальным поворотам, сильному боковому освещению, частичным скрытиям лица и плотным маскам/очкам; для более надёжной детекции желательно использовать более современный детектор (RetinaFace/RetinaFace-PyTorch, MTCNN или dlib CNN model).
- Биас датасета: готовые датасеты (UTKFace, APPA-REAL) имеют собственное распределение по возрасту/расе/половому признаку; модель может плохо обобщать на демографиях, не представленных в обучающей выборке.
- Возраст как аппроксимация: представление возраста как ожидаемого значения из распределения даёт сглаженную ошибку, но распределение может смещаться для пожилых/молодых крайних значений.
- Производительность: 
  - CPU-only инференс глубоких бэкбонов (EfficientNetB3, ResNet50) будет медленным; батчинг помогает, но для интерактивного демо GPU рекомендован.
  - dlib HOG быстрый на CPU, но менее точный по сравнению с CNN/RetinaFace; dlib CNN-модель даёт лучшее качество, но требует скачивания весов и GPU для приемлемой скорости.
- Тесты: есть базовые тесты в tests/, но покрытие функций инференса/edge-cases может быть ограничено.

---

5. Рекомендуемые улучшения
- Детекция и выравнивание:
  - заменить/опционально добавить RetinaFace или MTCNN для лучшей детекции в «wild» условиях; добавить выравнивание лица по ключевым точкам.
- Предобработка:
  - явно применять выравнивание/нормализацию освещения (CLAHE) и больше аугментаций при обучении (color jitter, blur, occlusion).
- Архитектура:
  - для мобильных/встраиваемых задач можно предложить MobileNetV3 или EfficientNet-lite; для точности — более глубокие backbones или ансамбли.
  - для возраста рассмотреть отдельные специализированные сети (regression head + distribution head) или двухэтапный подход (классификация в широкие группы + детальная регрессия внутри группы).
- Производительность/деплой:
  - экспорт моделей в ONNX/TFLite и применение квантования/пост-трейнинг оптимизации.
  - обеспечить batch inference и многопоточность загрузки изображений; использовать GPU-вычисления и tf.data pipeline.
- Метрики и валидация:
  - интегрировать систему оценки (unit tests и интеграционные тесты), добавить CI (GitHub Actions) для автоматического прогона тестов и линтинга.
  - провести анализ справедливости (fairness) по демографическим группам и добавить коррекцию/балансировку данных.
- UX/функционал:
  - веб-интерфейс / REST API для простого использования; добавить конфигурационные файлы/CLI через hydra и examples.
  - логирование прогонов экспериментов (wandb уже интегрирован, использовать чаще).

---

6. Быстрые инструкции для проверки (кратко)
- Установить venv и зависимости:
  - C:\Users\annef\Desktop\age-gender-estimation\venv\Scripts\activate
  - pip install -r requirements.txt
- Положить веса в локальную папку pretrained_models/ (например: pretrained_models/EfficientNetB3_224_weights.11-3.44.hdf5) или передать --weight_file при запуске.
- Для пакетной инференции:
  - python -m src.inference --image_dir test_images --output results.csv --save_crops --weight_file <путь_к_весу>
- Для оценки (при наличии meta/<db>.csv с ground truth):
  - python evaluate_predictions.py --results results.csv --db <db> --output metrics.json
- Для запуска тестов:
  - pytest -q

---

Заключение
Проект представляет собой современную пайплайн-реализацию определения пола и возраста: детекция (dlib) -> кроп/предобработка -> бэкбон (EfficientNet/ResNet/etc.) -> два хэда (gender, age distribution). Для воспроизводимости необходимо обеспечить локальные веса (pretrained_models/) и установить зависимости (cv2, dlib, tensorflow). Рекомендую начинать с обновления детектора (RetinaFace) и добавления оценки качества (evaluate_predictions) на реальных метках, а затем оптимизировать модель под целевую платформу (ONNX/TFLite/quantization).

Если хотите, могу:
- сгенерировать краткую презентацию (слайды) на основе этого отчёта; 
- или дополнительно запустить infer на test_images и приложить примеры результатов (если вы поместите локальные веса либо передадите путь к ним).